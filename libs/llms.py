#!/usr/bin/env python

##################################################################################################
#
# Copyright 2024, Shalin Garg
#
# This file is part of CodeDoc Gen AI Tool.
#
# CodeDoc is free software: you can redistribute it and/or modify it under the terms of the 
# GNU General Public License as published by the Free Software Foundation, either version 3 
# of the License, or (at your option) any later version.
#
# CodeDoc is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without 
# even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU 
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with CodeDoc. 
# If not, see <https://www.gnu.org/licenses/>.
#
##################################################################################################

__version__ = "0.1"
__author__  = "Shalin Garg"

import os
from typing import Any, Dict, List, Mapping, Optional

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from langchain.pydantic_v1 import Extra, root_validator
from langchain.utils import get_from_dict_or_env
from gradio_client import Client

__all__ = ["EnvVars", "HuggingFaceSpaces"]

DEFAULT_REPO_ID = "gpt2"
VALID_TASKS = ("text2text-generation", "text-generation", "summarization")

class EnvVars:
    @classmethod
    def checkEnviron(cls, provider:str):
        res = True
        if provider == "AzureOpenAI":
            res &= cls.checkVar("OPENAI_API_TYPE", provider)
            res &= cls.checkVar("OPENAI_API_VERSION", provider)
            res &= cls.checkVar("OPENAI_API_BASE", provider)
            res &= cls.checkVar("OPENAI_API_KEY", provider)
        elif provider == "OpenAI":
            res &= cls.checkVar("OPENAI_API_KEY", provider)
        elif provider == "AzureChatOpenAI":
            res &= cls.checkVar("AZURE_OPENAI_API_KEY", provider)
            res &= cls.checkVar("AZURE_OPENAI_ENDPOINT", provider)
            res &= cls.checkVar("OPENAI_API_VERSION", provider)
            res &= cls.checkVar("AZURE_DEPLOYMENT_NAME", provider)
        elif provider == "ChatGeminiPro":
            res &= cls.checkVar("GOOGLE_API_KEY", provider)
        
        return res
    
    @classmethod
    def checkVar(cls, varname:str, provider:str):
        if (os.getenv(varname)) is None:
            print(f"Env Var [{varname}] is required for provider [{provider}]")
            return False
        return True


class HuggingFaceSpaces(LLM):
    """HuggingFace Spaces.

    To use, you should have the ``gradio_client`` python package installed

    Only supports `text-generation`, `text2text-generation` and `summarization` for now.

    Example:
        .. code-block:: python

            import HuggingFaceSpaces
            hf = HuggingFaceSpaces(repo_id="gpt2")
    """

    client: Any  #: :meta private:
    repo_id: str = DEFAULT_REPO_ID
    """Model name to use."""

    task: Optional[str] = None
    """Task to call the model with.
    Should be a task that returns `generated_text` or `summary_text`."""

    model_kwargs: Optional[dict] = None
    """Keyword arguments to pass to the model."""

    huggingfacehub_api_token: Optional[str] = None

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that space exists on Hugging Face."""

        huggingfacehub_api_token = get_from_dict_or_env(
            values, "huggingfacehub_api_token", "HUGGINGFACEHUB_API_TOKEN", "default"
        )

        repo_id = values["repo_id"]
        client = Client(
            repo_id,
            huggingfacehub_api_token
        )

        if client.session_hash is None:
            raise ValueError(
                f"Got invalid spaces name {repo_id}"
            )
        
        values["client"] = client

        return values

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        
        _model_kwargs = self.model_kwargs or {}

        return {
            **{"repo_id": self.repo_id, "task": self.task},
            **{"model_kwargs": _model_kwargs},
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "huggingface_spaces"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:

        """Call out to HuggingFace Spaces' endpoint using Gradio Client.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = hf("Tell me a joke.")
        """

        _model_kwargs = self.model_kwargs or {}
        params = {'input':prompt, **_model_kwargs, **kwargs}
        #print(*tuple(params.values()))

        if 'api_name' in params:
            api_name = params.pop('api_name')
            response = self.client.predict(*params.values(), api_name=api_name)
        elif 'fn_index' in params:
            fn_index = params.pop('fn_index')
            response = self.client.predict(*params.values(), fn_index=fn_index)
        else:
            raise ValueError(f"api_name or fn_name required as params")

        if "error" in response and not isinstance(response, str):
            raise ValueError(f"Error raised by Gradio API: {response['error']}")

        if self.task == "text-generation":
            # Text generation return includes the starter text.
            text = response[len(prompt) :]
        elif self.task == "text2text-generation":
            text = response
        elif self.task == "summarization":
            text = response
        else:
            text = response
            #raise ValueError(
            #    f"Got invalid task {self.task}, "
            #    f"currently only {VALID_TASKS} are supported"
            #)

        if stop is not None:
            # This is a bit hacky, but I can't figure out a better way to enforce
            # stop tokens when making calls to huggingface_hub.
            text = enforce_stop_tokens(text, stop)
        return text


if __name__ == '__main__':
  print ('Cannot execute as a program, it is a module')
